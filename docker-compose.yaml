version: "3.8"

services:
  open-webui:
    depends_on:
      - ollama
    container_name: open-webui
    restart: unless-stopped
    image: ghcr.io/open-webui/open-webui:main
    environment:
      OLLAMA_BASE_URL: http://ollama:11434
      # WEBUI_SECRET_KEY: 
    ports:
      - 3030:8080
    volumes:
      - ./open-webui:/app/backend/data
    # extra_hosts:
    #   - "host.docker.internal:host-gateway"
    networks:
      - ollama_llm

  ollama:
    container_name: ollama
    restart: unless-stopped
    # image: ollama/ollama
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - 11434:11434 
    volumes:
      - ./ollama:/root/.ollama
    networks:
      - ollama_llm

# ollama list
# ollama pull llama3
# ollama show llama3:latest --modelfile > codefonteson.modelfile
# notepad codefonteson.modelfile

networks:
  ollama_llm:
    driver: bridge
